{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31e6def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opencc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb372f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {'<pad>': 0,\n",
    " '<eos>': 1,\n",
    " '(': 2,\n",
    " ')': 3,\n",
    " '*': 4,\n",
    " '+': 5,\n",
    " '-': 6,\n",
    " '0': 7,\n",
    " '1': 8,\n",
    " '2': 9,\n",
    " '3': 10,\n",
    " '4': 11,\n",
    " '5': 12,\n",
    " '6': 13,\n",
    " '7': 14,\n",
    " '8': 15,\n",
    " '9': 16,\n",
    " '=': 17,\n",
    " '<unk>': 18}\n",
    "id_to_char = {0: '<pad>',\n",
    " 1: '<eos>',\n",
    " 2: '(',\n",
    " 3: ')',\n",
    " 4: '*',\n",
    " 5: '+',\n",
    " 6: '-',\n",
    " 7: '0',\n",
    " 8: '1',\n",
    " 9: '2',\n",
    " 10: '3',\n",
    " 11: '4',\n",
    " 12: '5',\n",
    " 13: '6',\n",
    " 14: '7',\n",
    " 15: '8',\n",
    " 16: '9',\n",
    " 17: '=',\n",
    " 18: '<unk>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f55b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=embed_dim,\n",
    "                                            padding_idx=char_to_id['<pad>'])\n",
    "        \n",
    "        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=hidden_dim),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=hidden_dim,\n",
    "                                                          out_features=vocab_size))\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    # The forward pass of the model\n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        batch_x = self.embedding(batch_x)\n",
    "        \n",
    "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
    "                                                          batch_x_lens,\n",
    "                                                          batch_first=True,\n",
    "                                                          enforce_sorted=False)\n",
    "        \n",
    "        batch_x, _ = self.rnn_layer1(batch_x)\n",
    "        batch_x, _ = self.rnn_layer2(batch_x)\n",
    "        \n",
    "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,\n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        batch_x = self.linear(batch_x)\n",
    "        \n",
    "        return batch_x\n",
    "    \n",
    "    def generator(self, start_char, max_len=200):\n",
    "        \n",
    "        char_list = [char_to_id[c] for c in start_char]\n",
    "        answer_list = []\n",
    "        \n",
    "        next_char = None\n",
    "        \n",
    "        while len(char_list) < max_len: \n",
    "            # Write your code here \n",
    "            # Pack the char_list to tensor\n",
    "            input_tensor = torch.tensor([char_list], dtype=torch.long, device=self.embedding.weight.device)\n",
    "            input_lens = torch.tensor([len(char_list)], dtype=torch.long)\n",
    "            # Input the tensor to the embedding layer, LSTM layers, linear respectively\n",
    "            logits = self.encoder(input_tensor, input_lens)\n",
    "            # Obtain the next token prediction y\n",
    "            y = logits[0, -1, :]\n",
    "            \n",
    "            next_char = torch.argmax(y, dim=-1).item() # Use argmax function to get the next token prediction\n",
    "            \n",
    "            char_list.append(next_char)\n",
    "            \n",
    "            if next_char == char_to_id['<eos>']:\n",
    "                break\n",
    "            \n",
    "            \n",
    "        return [id_to_char[ch_id] for ch_id in char_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb9700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "lr = 0.001\n",
    "grad_clip = 1\n",
    "\n",
    "vocab_size = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db27c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharRNN(vocab_size,\n",
    "                embed_dim,\n",
    "                hidden_dim)\n",
    "model.load_state_dict(torch.load('model_weights.pth', map_location=torch.device('mps')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a58e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '+', '1', '=', '2', '<eos>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generator('1+1=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f53144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
